{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76df6676-6543-4894-adda-b7207b92b0ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS casino_ctg;\n",
    "USE catalog casino_ctg;\n",
    "CREATE SCHEMA IF NOT EXISTS log_db;\n",
    "USE schema log_db;\n",
    "CREATE VOLUME IF NOT EXISTS sourcefiles;\n",
    "TRUNCATE TABLE IF EXISTS transactions_bronze; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7e0a90b-08e0-480f-a073-f6ed6dc66a41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the target location in a Unity Catalog Volume\n",
    "volume_path = \"dbfs:/Volumes/casino_ctg/log_db/sourcefiles\"\n",
    "dbutils.fs.mkdirs(volume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb2ee2e-0328-487e-aaeb-622e9d51e7d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "import re\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Base path where all the raw CSVs were downloaded\n",
    "BASE_VOLUME_PATH = \"/Volumes/casino_ctg/log_db/sourcefiles\"\n",
    "TARGET_CATALOG = \"casino_ctg\"\n",
    "TARGET_SCHEMA = \"log_db\"\n",
    "DATE_COLUMN = 'date'\n",
    "TS_COLUMN ='timestamp'\n",
    "# 1. List all CSV files in the raw_files directory\n",
    "try:\n",
    "    # dbutils.fs.ls returns a list of FileInfo objects\n",
    "    file_list = dbutils.fs.ls(BASE_VOLUME_PATH)\n",
    "   # print(file_list)\n",
    "    trans_list =[ll for ll in file_list if ll.name.startswith('transactions')]\n",
    "except Exception as e:\n",
    "    print(f\"Error listing files in {BASE_VOLUME_PATH}: {e}\")\n",
    "    file_list = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4babf2e4-e2e7-4c99-b15f-5a838c1d6615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Column 'date' not found, skipping datetime transformation.\n  Column 'timestamp' not found, skipping datetime transformation.\n  Successfully wrote 4 rows to casino_ctg.log_db.games_bronze.\n"
     ]
    }
   ],
   "source": [
    "filename = 'games_metadata.csv'\n",
    "table_name = f\"games_bronze\"\n",
    "full_table_name = f\"{TARGET_CATALOG}.{TARGET_SCHEMA}.{table_name}\"\n",
    "raw_csv_path = f\"{BASE_VOLUME_PATH}/{filename}\"\n",
    "\n",
    "try:\n",
    "    # Load data using Spark DataFrame API to avoid /dbfs path access issues\n",
    "    df_spark = spark.read.option(\"header\", True).csv(raw_csv_path)\n",
    "    \n",
    "    # Check for the existence of the 'date' column and perform conversion if present\n",
    "    if DATE_COLUMN in df_spark.columns:\n",
    "        print(f\"  Transforming column '{DATE_COLUMN}' to datetime type.\")\n",
    "        df_spark = df_spark.withColumn(\n",
    "            DATE_COLUMN,\n",
    "            col(DATE_COLUMN).cast(\"timestamp\")\n",
    "        )\n",
    "    else:\n",
    "        print(f\"  Column '{DATE_COLUMN}' not found, skipping datetime transformation.\")\n",
    "\n",
    "    if TS_COLUMN in df_spark.columns:\n",
    "        print(f\"  Transforming column '{TS_COLUMN}' to datetime type.\")\n",
    "        df_spark = df_spark.withColumn(\n",
    "            TS_COLUMN,\n",
    "            col(TS_COLUMN).cast(\"timestamp\")\n",
    "        )\n",
    "    else:\n",
    "        print(f\"  Column '{TS_COLUMN}' not found, skipping datetime transformation.\")\n",
    "\n",
    "    # Write to the Bronze Delta Table using the UC naming convention\n",
    "    df_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "    \n",
    "    print(f\"  Successfully wrote {df_spark.count()} rows to {full_table_name}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  FAILED to process {filename}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06fb74a4-e82d-4d0d-850a-8b6842a69b6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Iterate over each file, process, and write to Bronze Delta Table\n",
    "for file_info in trans_list:\n",
    "    filename=file_info.name\n",
    "    table_name = f\"transactions_bronze\"\n",
    "    full_table_name = f\"{TARGET_CATALOG}.{TARGET_SCHEMA}.{table_name}\"\n",
    "    raw_csv_path = f\"{BASE_VOLUME_PATH}/{filename}\"\n",
    "    \n",
    "    print(f\"\\nProcessing file: {filename} -> Target table: {full_table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Load data using Spark DataFrame API to avoid /dbfs path access issues\n",
    "        df_spark = spark.read.option(\"header\", True).csv(raw_csv_path)\n",
    "        \n",
    "        # Check for the existence of the 'date' column and perform conversion if present\n",
    "        if DATE_COLUMN in df_spark.columns:\n",
    "            print(f\"  Transforming column '{DATE_COLUMN}' to datetime type.\")\n",
    "            df_spark = df_spark.withColumn(DATE_COLUMN, col(DATE_COLUMN).cast(\"timestamp\"))\n",
    "        else:\n",
    "            print(f\"  Column '{DATE_COLUMN}' not found, skipping datetime transformation.\")\n",
    "\n",
    "        if TS_COLUMN in df_spark.columns:\n",
    "            print(f\"  Transforming column '{TS_COLUMN}' to datetime type.\")\n",
    "            df_spark = df_spark.withColumn(TS_COLUMN, col(TS_COLUMN).cast(\"timestamp\"))\n",
    "        else:\n",
    "            print(f\"  Column '{TS_COLUMN}' not found, skipping datetime transformation.\")\n",
    "\n",
    "        # Write to the Bronze Delta Table using the UC naming convention\n",
    "        df_spark.write.format(\"delta\").mode(\"append\").saveAsTable(full_table_name)\n",
    "        \n",
    "        print(f\"  Successfully wrote {df_spark.count()} rows to {full_table_name}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  FAILED to process {filename}. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2350247026830673,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "load_casino_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}